{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"02_pytorch_classification_video.ipynb","provenance":[{"file_id":"https://github.com/mrdbourke/pytorch-deep-learning/blob/main/video_notebooks/02_pytorch_classification_video.ipynb","timestamp":1726609271278}],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# 02. Neural Network classification with PyTorch\n","\n","Classification is a problem of predicting whether something is one thing or another (there can be multiple things as the options).\n","\n","* Book version of this notebook - https://www.learnpytorch.io/02_pytorch_classification/\n","* All other resources - https://github.com/mrdbourke/pytorch-deep-learning\n","* Stuck? Ask a question - https://github.com/mrdbourke/pytorch-deep-learning/discussions"],"metadata":{"id":"BZ0Xn7qVenp8"}},{"cell_type":"markdown","source":["## 1. Make classification data and get it ready"],"metadata":{"id":"0exlIszJfBFB"}},{"cell_type":"code","source":["import sklearn"],"metadata":{"id":"ywkhYun3fW3a","executionInfo":{"status":"ok","timestamp":1726609270310,"user_tz":-180,"elapsed":692,"user":{"displayName":"","userId":""}}},"execution_count":82,"outputs":[]},{"cell_type":"code","source":["from sklearn.datasets import make_circles\n","\n","# Make 1000 samples\n","n_samples = 1000\n","\n","# Create circles\n","X, y = make_circles(n_samples,\n","                    noise=0.03,\n","                    random_state=42)"],"metadata":{"id":"o5ssZfgjfUC2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(X), len(y)"],"metadata":{"id":"96fM0MUYfrhg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f\"First 5 samples of X:\\n {X[:5]}\")\n","print(f\"First 5 samples of y:\\n {y[:5]}\")"],"metadata":{"id":"ew1Hds-dftUS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Make DataFrame of circle data\n","import pandas as pd\n","circles = pd.DataFrame({\"X1\": X[:, 0],\n","                        \"X2\": X[:, 1],\n","                        \"label\": y})\n","circles.head(10)"],"metadata":{"id":"yCU7lYZ9gFAf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["circles.label.value_counts()"],"metadata":{"id":"SBq9yzBj6rOH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualize, visualize, visualize\n","import matplotlib.pyplot as plt\n","plt.scatter(x=X[:, 0],\n","            y=X[:, 1],\n","            c=y,\n","            cmap=plt.cm.RdYlBu);"],"metadata":{"id":"ACyUNaFlglL0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Note:** The data we're working with is often referred to as a toy dataset, a dataset that is small enough to experiment but still sizeable enough to practice the fundamentals."],"metadata":{"id":"nTdbfhlehI9x"}},{"cell_type":"markdown","source":["### 1.1 Check input and output shapes"],"metadata":{"id":"08jHXdIKgyPF"}},{"cell_type":"code","source":["X.shape, y.shape"],"metadata":{"id":"8gOiNSewgyMo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X"],"metadata":{"id":"g_cR6eofgyKV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# View the first example of features and labels\n","X_sample = X[0]\n","y_sample = y[0]\n","\n","print(f\"Values for one sample of X: {X_sample} and the same for y: {y_sample}\")\n","print(f\"Shapes for one sample of X: {X_sample.shape} and the same for y: {y_sample.shape}\")"],"metadata":{"id":"qtCuPtJqgyID"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 1.2 Turn data into tensors and create train and test splits"],"metadata":{"id":"XJHKlEnBgx4a"}},{"cell_type":"code","source":["import torch\n","torch.__version__"],"metadata":{"id":"uR42UiPSgx1E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["type(X), X.dtype"],"metadata":{"id":"VM9Hw2wljGEm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Turn data into tensors\n","X = torch.from_numpy(X).type(torch.float)\n","y = torch.from_numpy(y).type(torch.float)\n","\n","X[:5], y[:5]"],"metadata":{"id":"3v5Q3sIQgxuk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["type(X), X.dtype, y.dtype"],"metadata":{"id":"QpttifEogxpI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Split data into training and test sets\n","from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, y_train, y_test = train_test_split(X,\n","                                                    y,\n","                                                    test_size=0.2, # 0.2 = 20% of data will be test & 80% will be train\n","                                                    random_state=42)"],"metadata":{"id":"cSDhwhz4i7mg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(X_train), len(X_test), len(y_train), len(y_test)"],"metadata":{"id":"4x1fQVaVi7jW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["n_samples"],"metadata":{"id":"ilZT21YrjgH9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2. Building a model\n","\n","Let's build a model to classify our blue and red dots.\n","\n","To do so, we want to:\n","1. Setup device agonistic code so our code will run on an accelerator (GPU) if there is one\n","2. Construct a model (by subclassing `nn.Module`)\n","3. Define a loss function and optimizer\n","4. Create a training and test loop"],"metadata":{"id":"L_NcXbrLjgEx"}},{"cell_type":"code","source":["# Import PyTorch and nn\n","import torch\n","from torch import nn\n","\n","# Make device agnostic code\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","device"],"metadata":{"id":"gntGKE7KE8sD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train"],"metadata":{"id":"_oyl1BdAE8qI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we've setup device agnostic code, let's create a model that:\n","\n","1. Subclasses `nn.Module` (almost all models in PyTorch subclass `nn.Module`)\n","2. Create 2 `nn.Linear()` layers that are capable of handling the shapes of our data\n","3. Defines a `forward()` method that outlines the forward pass (or forward computation) of the model\n","4. Instatiate an instance of our model class and send it to the target `device`"],"metadata":{"id":"XAuzqO7DjgBz"}},{"cell_type":"code","source":["X_train.shape"],"metadata":{"id":"7zEjsSZWHCWW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_train[:5]"],"metadata":{"id":"kNjO5I2LHXCd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn import datasets\n","# 1. Construct a model that subclasses nn.Module\n","class CircleModelV0(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    # 2. Create 2 nn.Linear layers capable of handling the shapes of our data\n","    self.layer_1 = nn.Linear(in_features=2, out_features=5) # takes in 2 features and upscales to 5 features\n","    self.layer_2 = nn.Linear(in_features=5, out_features=1) # takes in 5 features from previous layer and outputs a single feature (same shape as y)\n","\n","  # 3. Define a forward() method that outlines the forward pass\n","  def forward(self, x):\n","    return self.layer_2(self.layer_1(x)) # x -> layer_1 ->  layer_2 -> output\n","\n","# 4. Instantiate an instance of our model class and send it to the target device\n","model_0 = CircleModelV0().to(device)\n","model_0"],"metadata":{"id":"lIN-TOvYGZBf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device"],"metadata":{"id":"THHrI9iPGY_B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["next(model_0.parameters()).device"],"metadata":{"id":"VThEgNG_GY8k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Let's replicate the model above using nn.Sequential()\n","model_0 = nn.Sequential(\n","    nn.Linear(in_features=2, out_features=5),\n","    nn.Linear(in_features=5, out_features=1)\n",").to(device)\n","\n","model_0"],"metadata":{"id":"UW0ZATKnGY0f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_0.state_dict()"],"metadata":{"id":"nOKuT23HMUGd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Make predictions\n","with torch.inference_mode():\n","  untrained_preds = model_0(X_test.to(device))\n","print(f\"Length of predictions: {len(untrained_preds)}, Shape: {untrained_preds.shape}\")\n","print(f\"Length of test samples: {len(X_test)}, Shape: {X_test.shape}\")\n","print(f\"\\nFirst 10 predictions:\\n{torch.round(untrained_preds[:10])}\")\n","print(f\"\\nFirst 10 labels:\\n{y_test[:10]}\")"],"metadata":{"id":"h9HpguZ1k5LR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_test[:10], y_test[:10]"],"metadata":{"id":"DN7RbDmRk5Iy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 2.1 Setup loss function and optimizer\n","\n","Which loss function or optimizer should you use?\n","\n","Again... this is problem specific.\n","\n","For example for regression you might want MAE or MSE (mean absolute error or mean squared error).\n","\n","For classification you might want binary cross entropy or categorical cross entropy (cross entropy).\n","\n","As a reminder, the loss function measures how *wrong* your models predictions are.\n","\n","And for optimizers, two of the most common and useful are SGD and Adam, however PyTorch has many built-in options.\n","\n","* For some common choices of loss functions and optimizers - https://www.learnpytorch.io/02_pytorch_classification/#21-setup-loss-function-and-optimizer\n","* For the loss function we're going to use `torch.nn.BECWithLogitsLoss()`, for more on what binary cross entropy (BCE) is, check out this article - https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a\n","* For a defintion on what a logit is in deep learning - https://stackoverflow.com/a/52111173/7900723\n","* For different optimizers see `torch.optim`"],"metadata":{"id":"yLWkObPUk5GZ"}},{"cell_type":"code","source":["# Setup the loss function\n","# loss_fn = nn.BCELoss() # BCELoss = requires inputs to have gone through the sigmoid activation function prior to input to BCELoss\n","loss_fn = nn.BCEWithLogitsLoss() # BCEWithLogitsLoss = sigmoid activation function built-in\n","\n","optimizer = torch.optim.SGD(params=model_0.parameters(),\n","                            lr=0.1)"],"metadata":{"id":"XzlvBHAxk5D3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculate accuracy - out of 100 examples, what percentage does our model get right?\n","def accuracy_fn(y_true, y_pred):\n","  correct = torch.eq(y_true, y_pred).sum().item()\n","  acc = (correct/len(y_pred)) * 100\n","  return acc"],"metadata":{"id":"3yd16y0lT4Uq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3. Train model\n","\n","To train our model, we're going to need to build a training loop with the following steps:\n","\n","1. Forward pass\n","2. Calculate the loss\n","3. Optimizer zero grad\n","4. Loss backward (backpropagation)\n","5. Optimizer step (gradient descent)"],"metadata":{"id":"cF3KAKiuUath"}},{"cell_type":"markdown","source":["### 3.1 Going from raw logits -> prediction probabilities -> prediction labels\n","\n","Our model outputs are going to be raw **logits**.\n","\n","We can convert these **logits** into **prediction probabilities** by passing them to some kind of activation function (e.g. sigmoid for binary classification and softmax for multiclass classification).\n","\n","Then we can convert our model's prediction probabilities to **prediction labels** by either rounding them or taking the `argmax()`."],"metadata":{"id":"D2APTiyUV4LL"}},{"cell_type":"code","source":["# View the first 5 outputs of the forward pass on the test data\n","model_0.eval()\n","with torch.inference_mode():\n","  y_logits = model_0(X_test.to(device))[:5]\n","y_logits"],"metadata":{"id":"1Yg_6s5wWbnS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_test[:5]"],"metadata":{"id":"LiLz-TDzWg53"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Use the sigmoid activation function on our model logits to turn them into prediction probabilities\n","y_pred_probs = torch.sigmoid(y_logits)\n","y_pred_probs"],"metadata":{"id":"lrDZmTcZXiYu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["For our prediction probability values, we need to perform a range-style rounding on them:\n","* `y_pred_probs` >= 0.5, `y=1` (class 1)\n","* `y_pred_probs` < 0.5, `y=0` (class 0)"],"metadata":{"id":"f-vYzxKwX_G1"}},{"cell_type":"code","source":["# Find the predicted labels\n","y_preds = torch.round(y_pred_probs)\n","\n","# In full (logits -> pred probs -> pred labels)\n","y_pred_labels = torch.round(torch.sigmoid(model_0(X_test.to(device))[:5]))\n","\n","# Check for equality\n","print(torch.eq(y_preds.squeeze(), y_pred_labels.squeeze()))\n","\n","# Get rid of extra dimension\n","y_preds.squeeze()"],"metadata":{"id":"6WQb8sp_Xwey"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_test[:5]"],"metadata":{"id":"jXkhLOT5X1wq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 3.2 Building a training and testing loop"],"metadata":{"id":"u7pb-y3OZG5b"}},{"cell_type":"code","source":["torch.manual_seed(42)\n","torch.cuda.manual_seed(42)\n","\n","# Set the number of epochs\n","epochs = 100\n","\n","# Put data to target device\n","X_train, y_train = X_train.to(device), y_train.to(device)\n","X_test, y_test = X_test.to(device), y_test.to(device)\n","\n","# Build training and evaluation loop\n","for epoch in range(epochs):\n","  ### Training\n","  model_0.train()\n","\n","  # 1. Forward pass\n","  y_logits = model_0(X_train).squeeze()\n","  y_pred = torch.round(torch.sigmoid(y_logits)) # turn logits -> pred probs -> pred labels\n","\n","  # 2. Calculate loss/accuracy\n","  # loss = loss_fn(torch.sigmoid(y_logits), # nn.BCELoss expects prediction probabilities as input\n","  #                y_train)\n","  loss = loss_fn(y_logits, # nn.BCEWithLogitsLoss expects raw logits as input\n","                 y_train)\n","  acc = accuracy_fn(y_true=y_train,\n","                    y_pred=y_pred)\n","\n","  # 3. Optimizer zero grad\n","  optimizer.zero_grad()\n","\n","  # 4. Loss backward (backpropagation)\n","  loss.backward()\n","\n","  # 5. Optimizer step (gradient descent)\n","  optimizer.step()\n","\n","  ### Testing\n","  model_0.eval()\n","  with torch.inference_mode():\n","    # 1. Forward pass\n","    test_logits = model_0(X_test).squeeze()\n","    test_pred = torch.round(torch.sigmoid(test_logits))\n","\n","    # 2. Calculate test loss/acc\n","    test_loss = loss_fn(test_logits,\n","                        y_test)\n","    test_acc = accuracy_fn(y_true=y_test,\n","                           y_pred=test_pred)\n","\n","  # Print out what's happenin'\n","  if epoch % 10 == 0:\n","    print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Acc: {acc:.2f}% | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\")"],"metadata":{"id":"DxUjouKwZMRl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 4. Make predictions and evaluate the model\n","\n","From the metrics it looks like our model isn't learning anything...\n","\n","So to inspect it let's make some predictions and make them visual!\n","\n","In other words, \"Visualize, visualize, visualize!\"\n","\n","To do so, we're going to import a function called `plot_decision_boundary()` - https://github.com/mrdbourke/pytorch-deep-learning/blob/main/helper_functions.py"],"metadata":{"id":"ud5xW_kM3xtH"}},{"cell_type":"code","source":["import requests\n","from pathlib import Path\n","\n","# Download helper functions from Learn PyTorch repo (if it's not already downloaded)\n","if Path(\"helper_functions.py\").is_file():\n","  print(\"helper_functions.py already exists, skipping download\")\n","else:\n","  print(\"Downloading helper_functions.py\")\n","  request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n","  with open(\"helper_functions.py\", \"wb\") as f:\n","    f.write(request.content)\n","\n","from helper_functions import plot_predictions, plot_decision_boundary"],"metadata":{"id":"NhFdb1RX7Flu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plot decision boundary of the model\n","plt.figure(figsize=(12, 6))\n","plt.subplot(1, 2, 1)\n","plt.title(\"Train\")\n","plot_decision_boundary(model_0, X_train, y_train)\n","plt.subplot(1, 2, 2)\n","plt.title(\"Test\")\n","plot_decision_boundary(model_0, X_test, y_test)"],"metadata":{"id":"8V7KyDQ78YxA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 5. Improving a model (from a model perspective)\n","\n","* Add more layers - give the model more chances to learn about patterns in the data\n","* Add more hidden units - go from 5 hidden units to 10 hidden units\n","* Fit for longer\n","* Changing the activation functions\n","* Change the learning rate\n","* Change the loss function\n","\n","These options are all from a model's perspective because they deal directly with the model, rather than the data.\n","\n","And because these options are all values we (as machine learning engineers and data scientists) can change, they are referred as **hyperparameters**.\n","\n","Let's try and improve our model by:\n","* Adding more hidden units: 5 -> 10\n","* Increase the number of layers: 2 -> 3\n","* Increase the number of epochs: 100 -> 1000"],"metadata":{"id":"RYn2XJ6I9BU3"}},{"cell_type":"code","source":["X_train[:5], y_train[:5]"],"metadata":{"id":"2HyQySuuBcH0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create a model\n","class CircleModelV1(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    self.layer_1 = nn.Linear(in_features=2, out_features=10)\n","    self.layer_2 = nn.Linear(in_features=10, out_features=10)\n","    self.layer_3 = nn.Linear(in_features=10, out_features=1)\n","\n","  def forward(self, x):\n","    # z = self.layer_1(x)\n","    # z = self.layer_2(z)\n","    # z = self.layer_3(z)\n","    return self.layer_3(self.layer_2(self.layer_1(x))) # this way of writing operations leverages speed ups where possible behind the scenes\n","\n","model_1 = CircleModelV1().to(device)\n","model_1"],"metadata":{"id":"7l3EDkwm-EPe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create a loss function\n","loss_fn = nn.BCEWithLogitsLoss()\n","\n","# Create an optimizer\n","optimizer = torch.optim.SGD(params=model_1.parameters(),\n","                            lr=0.1)"],"metadata":{"id":"T_6K4KoD-MOS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Write a training and evaluation loop for model_1\n","torch.manual_seed(42)\n","torch.cuda.manual_seed(42)\n","\n","# Train for longer\n","epochs = 1000\n","\n","# Put data on the target device\n","X_train, y_train = X_train.to(device), y_train.to(device)\n","X_test, y_test = X_test.to(device), y_test.to(device)\n","\n","for epoch in range(epochs):\n","  ### Training\n","  model_1.train()\n","  # 1. Forward pass\n","  y_logits = model_1(X_train).squeeze()\n","  y_pred = torch.round(torch.sigmoid(y_logits)) # logits -> pred probabilities -> prediction labels\n","\n","  # 2. Calculate the loss/acc\n","  loss = loss_fn(y_logits, y_train)\n","  acc = accuracy_fn(y_true=y_train,\n","                    y_pred=y_pred)\n","\n","  # 3. Optimizer zero grad\n","  optimizer.zero_grad()\n","\n","  # 4. Loss backward (backpropagation)\n","  loss.backward()\n","\n","  # 5. Optimizer step (gradient descent)\n","  optimizer.step()\n","\n","  ### Testing\n","  model_1.eval()\n","  with torch.inference_mode():\n","    # 1. Forward pass\n","    test_logits = model_1(X_test).squeeze()\n","    test_pred = torch.round(torch.sigmoid(test_logits))\n","    # 2. Calculate loss\n","    test_loss = loss_fn(test_logits,\n","                        y_test)\n","    test_acc = accuracy_fn(y_true=y_test,\n","                           y_pred=test_pred)\n","\n","  # Print out what's happenin'\n","  if epoch % 100 == 0:\n","    print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Acc: {acc:.2f}% | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\")"],"metadata":{"id":"sCVLN4FiCcnn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plot decision boundary of the model\n","plt.figure(figsize=(12, 6))\n","plt.subplot(1, 2, 1)\n","plt.title(\"Train\")\n","plot_decision_boundary(model_1, X_train, y_train)\n","plt.subplot(1, 2, 2)\n","plt.title(\"Test\")\n","plot_decision_boundary(model_1, X_test, y_test)"],"metadata":{"id":"NJp4kN83EiVt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 5.1 Preparing data to see if our model can fit a straight line\n","\n","One way to troubleshoot to a larger problem is to test out a smaller problem."],"metadata":{"id":"Le-cKAwaEiSi"}},{"cell_type":"code","source":["# Create some data (same as notebook 01)\n","weight = 0.7\n","bias = 0.3\n","start = 0\n","end = 1\n","step = 0.01\n","\n","# Create data\n","X_regression = torch.arange(start, end, step).unsqueeze(dim=1)\n","y_regression = weight * X_regression + bias # Linear regression formula (without epsilon)\n","\n","# Check the data\n","print(len(X_regression))\n","X_regression[:5], y_regression[:5]"],"metadata":{"id":"PGabtpZ-EiPu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create train and test splits\n","train_split = int(0.8 * len(X_regression))\n","X_train_regression, y_train_regression = X_regression[:train_split], y_regression[:train_split]\n","X_test_regression, y_test_regression = X_regression[train_split:], y_regression[train_split:]\n","\n","# Check the lengths of each\n","len(X_train_regression), len(X_test_regression), len(y_train_regression), len(y_test_regression)"],"metadata":{"id":"YSrnNiFWEiMa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_predictions(train_data=X_train_regression,\n","                 train_labels=y_train_regression,\n","                 test_data=X_test_regression,\n","                 test_labels=y_test_regression);"],"metadata":{"id":"wG1Dhz9SEiJD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 5.2 Adjusting `model_1` to fit a straight line"],"metadata":{"id":"usxOtv8tGlaZ"}},{"cell_type":"code","source":["# Same architecture as model_1 (but using nn.Sequential())\n","model_2 = nn.Sequential(\n","    nn.Linear(in_features=1, out_features=10),\n","    nn.Linear(in_features=10, out_features=10),\n","    nn.Linear(in_features=10, out_features=1)\n",").to(device)\n","\n","model_2"],"metadata":{"id":"sRUPRukiGlYO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Loss and optimizer\n","loss_fn = nn.L1Loss() # MAE loss with regression data\n","optimizer = torch.optim.SGD(params=model_2.parameters(),\n","                            lr=0.01)"],"metadata":{"id":"twGWjcG4GlWJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Train the model\n","torch.manual_seed(42)\n","torch.cuda.manual_seed(42)\n","\n","# Set the number of epochs\n","epochs = 1000\n","\n","# Put the data on the target device\n","X_train_regression, y_train_regression = X_train_regression.to(device), y_train_regression.to(device)\n","X_test_regression, y_test_regression = X_test_regression.to(device), y_test_regression.to(device)\n","\n","# Training\n","for epoch in range(epochs):\n","  y_pred = model_2(X_train_regression)\n","  loss = loss_fn(y_pred, y_train_regression)\n","  optimizer.zero_grad()\n","  loss.backward()\n","  optimizer.step()\n","\n","  # Testing\n","  model_2.eval()\n","  with torch.inference_mode():\n","    test_pred = model_2(X_test_regression)\n","    test_loss = loss_fn(test_pred, y_test_regression)\n","\n","  # Print out what's happenin'\n","  if epoch % 100 == 0:\n","    print(f\"Epoch: {epoch} | Loss: {loss:.5f} | Test loss: {test_loss:.5f}\")"],"metadata":{"id":"9cTT35QgkPP-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Turn on evaluation mode\n","model_2.eval()\n","\n","# Make predictions (inference)\n","with torch.inference_mode():\n","  y_preds = model_2(X_test_regression)\n","\n","# Plot data and predictions\n","plot_predictions(train_data=X_train_regression.cpu(),\n","                 train_labels=y_train_regression.cpu(),\n","                 test_data=X_test_regression.cpu(),\n","                 test_labels=y_test_regression.cpu(),\n","                 predictions=y_preds.cpu());"],"metadata":{"id":"TiaYv8p-lPV2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 6. The missing piece: non-linearity\n","\n","\"What patterns could you draw if you were given an infinite amount of a straight and non-straight lines?\"\n","\n","Or in machine learning terms, an infinite (but really it is finite) of linear and non-linear functions?"],"metadata":{"id":"HMvNNwSJmm98"}},{"cell_type":"markdown","source":["### 6.1 Recreating non-linear data (red and blue circles)"],"metadata":{"id":"j_P9_-1apMWq"}},{"cell_type":"code","source":["# Make and plot data\n","import matplotlib.pyplot as plt\n","from sklearn.datasets import make_circles\n","\n","n_samples = 1000\n","\n","X, y = make_circles(n_samples,\n","                    noise=0.03,\n","                    random_state=42)\n","\n","plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdYlBu);"],"metadata":{"id":"TfUWJPWBpo4-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Convert data to tensors and then to train and test splits\n","import torch\n","from sklearn.model_selection import train_test_split\n","\n","# Turn data into tensors\n","X = torch.from_numpy(X).type(torch.float)\n","y = torch.from_numpy(y).type(torch.float)\n","\n","# Split into train and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X,\n","                                                    y,\n","                                                    test_size=0.2,\n","                                                    random_state=42)\n","\n","X_train[:5], y_train[:5]"],"metadata":{"id":"4pocAXPCp7o2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 6.2 Building a model with non-linearity\n","\n","* Linear = straight lines\n","* Non-linear = non-straight lines\n","\n","Artificial neural networks are a large combination of linear (straight) and non-straight (non-linear) functions which are potentially able to find patterns in data."],"metadata":{"id":"TbVSE-VeqjsG"}},{"cell_type":"code","source":["# Build a model with non-linear activation functions\n","from torch import nn\n","class CircleModelV2(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    self.layer_1 = nn.Linear(in_features=2, out_features=10)\n","    self.layer_2 = nn.Linear(in_features=10, out_features=10)\n","    self.layer_3 = nn.Linear(in_features=10, out_features=1)\n","    self.relu = nn.ReLU() # relu is a non-linear activation function\n","\n","  def forward(self, x):\n","    # Where should we put our non-linear activation functions?\n","    return self.layer_3(self.relu(self.layer_2(self.relu(self.layer_1(x)))))\n","\n","model_3 = CircleModelV2().to(device)\n","model_3"],"metadata":{"id":"ENa01YTsF0Pt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Setup loss and optimizer\n","loss_fn = nn.BCEWithLogitsLoss()\n","optimizer = torch.optim.SGD(model_3.parameters(),\n","                            lr=0.1)"],"metadata":{"id":"vUSJUD9AHyqZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 6.3 Training a model with non-linearity"],"metadata":{"id":"32forBUPJcTd"}},{"cell_type":"code","source":["len(X_test), len(y_test)"],"metadata":{"id":"-C0CpzWULlCo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Random seeds\n","torch.manual_seed(42)\n","torch.cuda.manual_seed(42)\n","\n","# Put all data on target device\n","X_train, y_train = X_train.to(device), y_train.to(device)\n","X_test, y_test = X_test.to(device), y_test.to(device)\n","\n","# Loop through data\n","epochs = 1000\n","\n","for epoch in range(epochs):\n","  ### Training\n","  model_3.train()\n","\n","  # 1. Forward pass\n","  y_logits = model_3(X_train).squeeze()\n","  y_pred = torch.round(torch.sigmoid(y_logits)) # logits -> prediction probabilities -> prediction labels\n","\n","  # 2. Calculate the loss\n","  loss = loss_fn(y_logits, y_train) # BCEWithLogitsLoss (takes in logits as first input)\n","  acc = accuracy_fn(y_true=y_train,\n","                    y_pred=y_pred)\n","\n","  # 3. Optimizer zero grad\n","  optimizer.zero_grad()\n","\n","  # 4. Loss backward\n","  loss.backward()\n","\n","  # 5. Step the optimizer\n","  optimizer.step()\n","\n","  ### Testing\n","  model_3.eval()\n","  with torch.inference_mode():\n","    test_logits = model_3(X_test).squeeze()\n","    test_pred = torch.round(torch.sigmoid(test_logits))\n","\n","    test_loss = loss_fn(test_logits, y_test)\n","    test_acc = accuracy_fn(y_true=y_test,\n","                           y_pred=test_pred)\n","\n","  # Print out what's this happenin'\n","  if epoch % 100 == 0:\n","    print(f\"Epoch: {epoch} | Loss: {loss:.4f}, Acc: {acc:.2f}% | Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%\")"],"metadata":{"id":"kriIowDxJn0J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 6.4 Evaluating a model trained with non-linear activation functions"],"metadata":{"id":"MYNiJIEtLCZt"}},{"cell_type":"code","source":["# Makes predictions\n","model_3.eval()\n","with torch.inference_mode():\n","  y_preds = torch.round(torch.sigmoid(model_3(X_test))).squeeze()\n","y_preds[:10], y_test[:10]"],"metadata":{"id":"yX2i_RQtLCXC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plot decision boundaries\n","plt.figure(figsize=(12, 6))\n","plt.subplot(1, 2, 1)\n","plt.title(\"Train\")\n","plot_decision_boundary(model_1, X_train, y_train) # model_1 = no non-linearity\n","plt.subplot(1, 2, 2)\n","plt.title(\"Test\")\n","plot_decision_boundary(model_3, X_test, y_test) # model_3 = has non-linearity"],"metadata":{"id":"TNm377UdLCUP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Challenge:** Can you improve model_3 to do better than 80% accuracy on the test data?"],"metadata":{"id":"cVJLKGWzLCRb"}},{"cell_type":"markdown","source":["## 7. Replicating non-linear activation functions\n","\n","Neural networks, rather than us telling the model what to learn, we give it the tools to discover patterns in data and it tries to figure out the patterns on its own.\n","\n","And these tools are linear & non-linear functions."],"metadata":{"id":"TGHd76BYNmec"}},{"cell_type":"code","source":["# Create a tensor\n","A = torch.arange(-10, 10, 1, dtype=torch.float32)\n","A.dtype"],"metadata":{"id":"XjXBtkWLNmaF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["A"],"metadata":{"id":"7wA4ZpNyJu9b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualize the tensor\n","plt.plot(A);"],"metadata":{"id":"X3r8BSjGO8g_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.plot(torch.relu(A));"],"metadata":{"id":"Gp_tFktUO8eI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["A"],"metadata":{"id":"CKmMlFW1PvK9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def relu(x: torch.Tensor) -> torch.Tensor:\n","  return torch.maximum(torch.tensor(0), x) # inputs must be tensors\n","\n","relu(A)"],"metadata":{"id":"_hdk3VY_O8a9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plot ReLU activation function\n","plt.plot(relu(A));"],"metadata":{"id":"j29MXDwKPh1U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Now let's do the same for Sigmoid = https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html#torch.nn.Sigmoid\n","def sigmoid(x):\n","  return 1 / (1 + torch.exp(-x))"],"metadata":{"id":"FXR9tfc3Phwc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.plot(torch.sigmoid(A));"],"metadata":{"id":"j5F30-gHP8Lw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.plot(sigmoid(A));"],"metadata":{"id":"68OTxhZmPhsN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 8. Putting it all together with a multi-class classification problem\n","\n","* Binary classification = one thing or another (cat vs. dog, spam vs. not spam, fraud or not fraud)\n","* Multi-class classification = more than one thing or another (cat vs. dog vs. chicken)"],"metadata":{"id":"OpHIZK9CPha_"}},{"cell_type":"markdown","source":["### 8.1 Creating a toy multi-class dataset"],"metadata":{"id":"R_pdUtYsR4Um"}},{"cell_type":"code","source":["# Import dependencies\n","import torch\n","import matplotlib.pyplot as plt\n","from sklearn.datasets import make_blobs # https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html#sklearn.datasets.make_blobs\n","from sklearn.model_selection import train_test_split\n","\n","# Set the hyperparameters for data creation\n","NUM_CLASSES = 4\n","NUM_FEATURES = 2\n","RANDOM_SEED = 42\n","\n","# 1. Create multi-class data\n","X_blob, y_blob = make_blobs(n_samples=1000,\n","                            n_features=NUM_FEATURES,\n","                            centers=NUM_CLASSES,\n","                            cluster_std=1.5, # give the clusters a little shake up\n","                            random_state=RANDOM_SEED)\n","\n","# 2. Turn data into tensors\n","X_blob = torch.from_numpy(X_blob).type(torch.float)\n","y_blob = torch.from_numpy(y_blob).type(torch.LongTensor)\n","\n","# 3. Split into train and test\n","X_blob_train, X_blob_test, y_blob_train, y_blob_test = train_test_split(X_blob,\n","                                                                        y_blob,\n","                                                                        test_size=0.2,\n","                                                                        random_state=RANDOM_SEED)\n","\n","# 4. Plot data (visualize, visualize, visualize)\n","plt.figure(figsize=(10, 7))\n","plt.scatter(X_blob[:, 0], X_blob[:, 1], c=y_blob, cmap=plt.cm.RdYlBu);"],"metadata":{"id":"G3rlb9omR7dx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 8.2 Building a multi-class classification model in PyTorch"],"metadata":{"id":"cxa4vNR0R7ax"}},{"cell_type":"code","source":["# Create device agnostic code\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","device"],"metadata":{"id":"Is3TPvOIR7SX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Build a multi-class classification model\n","class BlobModel(nn.Module):\n","  def __init__(self, input_features, output_features, hidden_units=8):\n","    \"\"\"Initializes multi-class classification model.\n","\n","    Args:\n","      input_features (int): Number of input features to the model\n","      output_features (int): Number of outputs features (number of output classes)\n","      hidden_units (int): Number of hidden units between layers, default 8\n","\n","    Returns:\n","\n","    Example:\n","    \"\"\"\n","    super().__init__()\n","    self.linear_layer_stack = nn.Sequential(\n","        nn.Linear(in_features=input_features, out_features=hidden_units),\n","        # nn.ReLU(),\n","        nn.Linear(in_features=hidden_units, out_features=hidden_units),\n","        # nn.ReLU(),\n","        nn.Linear(in_features=hidden_units, out_features=output_features)\n","    )\n","\n","  def forward(self, x):\n","    return self.linear_layer_stack(x)\n","\n","# Create an instance of BlobModel and send it to the target device\n","model_4 = BlobModel(input_features=2,\n","                    output_features=4,\n","                    hidden_units=8).to(device)\n","\n","model_4"],"metadata":{"id":"4KYMrjuCdHHb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_blob_train.shape, y_blob_train[:5]"],"metadata":{"id":"MFjKF1qHdHBv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.unique(y_blob_train)"],"metadata":{"id":"7cnvNS0YdGNV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 8.3 Create a loss function and an optimizer for a multi-class classification model"],"metadata":{"id":"0zNAjKSaelX4"}},{"cell_type":"code","source":["# Create a loss function for multi-class classification - loss function measures how wrong our model's predictions are\n","loss_fn = nn.CrossEntropyLoss()\n","\n","# Create an optimizer for multi-class classification - optimizer updates our model parameters to try and reduce the loss\n","optimizer = torch.optim.SGD(params=model_4.parameters(),\n","                            lr=0.1) # learning rate is a hyperparameter you can change"],"metadata":{"id":"npZosTbqf8jI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 8.4 Getting prediction probabilities for a multi-class PyTorch model\n","\n","In order to evaluate and train and test our model, we need to convert our model's outputs (logtis) to predicition probabilities and then to prediction labels.\n","\n","Logits (raw output of the model) -> Pred probs (use `torch.softmax`) -> Pred labels (take the argmax of the prediction probabilities)"],"metadata":{"id":"hc8BN4MghEpG"}},{"cell_type":"code","source":["# Let's get some raw outputs of our model (logits)\n","model_4.eval()\n","with torch.inference_mode():\n","  y_logits = model_4(X_blob_test.to(device))\n","\n","y_logits[:10]"],"metadata":{"id":"r3c3XxhShMnO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_blob_test[:10]"],"metadata":{"id":"WKQwb_4aiIxn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Convert our model's logit outputs to prediction probabilities\n","y_pred_probs = torch.softmax(y_logits, dim=1)\n","print(y_logits[:5])\n","print(y_pred_probs[:5])"],"metadata":{"id":"dBPtpiGjhyEP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_pred_probs.shape"],"metadata":{"id":"KNu1rUWR3Mye"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Convert our model's prediction probabilities to prediction labels\n","y_preds = torch.argmax(y_pred_probs, dim=1)\n","y_preds"],"metadata":{"id":"vkH214Emh_Im"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_blob_test"],"metadata":{"id":"mzRST_n8i8yV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 8.5 Creating a training loop and testing loop for a multi-class PyTorch model"],"metadata":{"id":"vNUDES0mi8v-"}},{"cell_type":"code","source":["y_blob_train.dtype"],"metadata":{"id":"TfFzD8T0m49e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Fit the multi-class model to the data\n","torch.manual_seed(42)\n","torch.cuda.manual_seed(42)\n","\n","# Set number of epochs\n","epochs = 100\n","\n","# Put data to the target device\n","X_blob_train, y_blob_train = X_blob_train.to(device), y_blob_train.to(device)\n","X_blob_test, y_blob_test = X_blob_test.to(device), y_blob_test.to(device)\n","\n","# Loop through data\n","for epoch in range(epochs):\n","  ### Training\n","  model_4.train()\n","\n","  y_logits = model_4(X_blob_train)\n","  y_pred = torch.softmax(y_logits, dim=1).argmax(dim=1)\n","\n","  loss = loss_fn(y_logits, y_blob_train)\n","  acc = accuracy_fn(y_true=y_blob_train,\n","                    y_pred=y_pred)\n","\n","  optimizer.zero_grad()\n","  loss.backward()\n","  optimizer.step()\n","\n","  ### Testing\n","  model_4.eval()\n","  with torch.inference_mode():\n","    test_logits = model_4(X_blob_test)\n","    test_preds = torch.softmax(test_logits, dim=1).argmax(dim=1)\n","\n","    test_loss = loss_fn(test_logits, y_blob_test)\n","    test_acc = accuracy_fn(y_true=y_blob_test,\n","                           y_pred=test_preds)\n","\n","  # Print out what's happenin'\n","  if epoch % 10 == 0:\n","    print(f\"Epoch: {epoch} | Loss: {loss:.4f}, Acc: {acc:.2f}% | Test loss: {test_loss:.4f}, Test acc: {test_acc:.2f}%\")"],"metadata":{"id":"CzQG_INbi8tG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 8.6 Making and evaluating predictions with a PyTorch multi-class model"],"metadata":{"id":"7GsIPbD3l81j"}},{"cell_type":"code","source":["# Make predictions\n","model_4.eval()\n","with torch.inference_mode():\n","  y_logits = model_4(X_blob_test)\n","\n","# View the first 10 predictions\n","y_logits[:10]"],"metadata":{"id":"bcLD0Iqal84o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Go from logits -> Prediction probabilities\n","y_pred_probs = torch.softmax(y_logits, dim=1)\n","y_pred_probs[:10]"],"metadata":{"id":"Nsn_2gE5l9SG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Go from pred probs to pred labels\n","y_preds = torch.argmax(y_pred_probs, dim=1)\n","y_preds[:10]"],"metadata":{"id":"J7fXmpC-l9PI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(12, 6))\n","plt.subplot(1, 2, 1)\n","plt.title(\"Train\")\n","plot_decision_boundary(model_4, X_blob_train, y_blob_train)\n","plt.subplot(1, 2, 2)\n","plt.title(\"Test\")\n","plot_decision_boundary(model_4, X_blob_test, y_blob_test)"],"metadata":{"id":"cvciOiS6q0Jo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 9. A few more classification metrics... (to evaluate our classification model)\n","\n","* Accuracy - out of 100 samples, how many does our model get right?\n","* Precision\n","* Recall\n","* F1-score\n","* Confusion matrix\n","* Classification report\n","\n","See this article for when to use precision/recall - https://towardsdatascience.com/beyond-accuracy-precision-and-recall-3da06bea9f6c\n","\n","If you want access to a lot of PyTorch metrics, see TorchMetrics - https://torchmetrics.readthedocs.io/en/latest/"],"metadata":{"id":"YyD62KSPq0HD"}},{"cell_type":"code","source":["!pip install torchmetrics"],"metadata":{"id":"zUPfxaG7l869"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torchmetrics import Accuracy\n","\n","# Setup metric\n","torchmetric_accuracy = Accuracy().to(device)\n","\n","# Calculuate accuracy\n","torchmetric_accuracy(y_preds, y_blob_test)"],"metadata":{"id":"YsKTgOmhlqyJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torchmetric_accuracy.device"],"metadata":{"id":"_cST55cwuU4V"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Exercises & Extra-curriculum\n","\n","See exercises and extra-curriculum here: https://www.learnpytorch.io/02_pytorch_classification/#exercises"],"metadata":{"id":"vf_cd5t5uZQh"}},{"cell_type":"code","source":[],"metadata":{"id":"ycgmZrSGvf_l"},"execution_count":null,"outputs":[]}]}